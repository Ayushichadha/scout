# ARC training config

defaults:
  - arch: hrm_v1
  - _self_

hydra:
  output_subdir: null

# Data path
data_path: data/arc-aug-1000

# Runtime
device: cuda        # override with device=cpu for CPU-only
max_steps: null     # optional clamp for smoke tests
enable_wandb: true  # set enable_wandb=false to disable logging
seed: 0

# Quick-mode helpers (safe tiny defaults)
quick_overfit: false
quick_max_steps: 15
quick_batch_size: 16
quick_arch:
  hidden_size: 32
  n_layers: 1
  n_heads: 2

# Hyperparams - Training
global_batch_size: 768

epochs: 100000
eval_interval: 10000
checkpoint_every_eval: True

# Debugging / sanity checks
overfit_one_batch: false

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings training
puzzle_emb_lr: 1e-2
